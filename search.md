@def title = "Search"

~~~
<script>
const mystuff = [
{title: "Animating a list shuffle with the FLIP technique", content: ", but I've linked some resources below.  FLIP stands for First Last Invert Play. It's a technique for easily animating objects that are moving around. You record the **first** position of the object, and the **last** position of your object (after the animation), then **invert** the object's position to the first position using a transformation. Finally, **\"play\"** an animation which transitions the object's position from the inverted transform to the identity transform.  [This article](https://css-tricks.com/animating-layouts-with-the-flip-technique/) by css tricks has the best explanation I've found. The css tricks article links to the original article [here](https://aerotwist.com/blog/flip-your-animations/). I found [two](https://medium.com/developers-writing/animating-the-unanimatable-1346a5aab3cd) [articles](https://www.taraojo.com/post/animating-element-reordering) showing how to integrate it into React's framework but nothing about vanilla html/js. ### Why I chose vanilla html/js  I chose to use vanilla html/js instead of React since I was auto-generating the html content and I didn't want to have to deal with the dependency of React. I found that chatGPT couldn't write correct code snippets using vanilla js, which indicated to me that there aren't many examples on the web.  It was a lot of trial and error to figure out how the browser schedules animation frames and when to change the style on each element. It turns out the best way to do it is rearrange all the elements on the DOM. Then, in a `requestAnimationFrame` thunk, do the inversion (animate them to their old positions with 0ms transition). Next, force the browser to do the animation with `void container.offsetHeight;` (which is browser magic). Then play them by setting their transform to nothing with a transition of `300ms`.  The `get_val` function takes the item and converts it to a real number which we use for sorting. I chose to put the values I wanted to sort by as fields of the element (and so `getAttribute(data-${indexType})` correctly retrieves it). But, you could choose any implementation/storage for the sorting keys.  If we're willing to treat the `void container.offsetHeight;` as a black box then everything should make sense. My guess is that if you ask for the offset height ([which is](https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetHeight) a measurement in pixels of the element's CSS height, including any borders, padding, and horizontal scrollbars), then it will force the browser to move all the elements to their animated locations.  Below is the code. Replace the `...` placeholders with whatever data you want.  html <div class=\"container\"> <details data-xvalue=\"186\" data-idnumber=\"148\">   <summary>Item title</summary>   <pre>...</pre> </details> ... </div>   javascript function sortDetails(indexType) {   // get a list of items to sort   const container = document.querySelector('.container');   const items = Array.from(container.children);    // 1. FIRST   const oldPositions = items.map(el => el.getBoundingClientRect());    // rearrange the items on the div   const get_val = (x => parseFloat(x.getAttribute(`data-${indexType}`)));   const argsort = Array(items.length).fill(0).map((_, i) => i).sort((i, j) =>     get_val(items[i]) - get_val(items[j])   )   argsort.forEach(i => container.appendChild(items[i]));    // 2. LAST   const newPositions = items.map(el => el.getBoundingClientRect());    requestAnimationFrame(() => {     // 3. INVERT     items.forEach((el, i) => {   	  const dx = oldPositions[i].left - newPositions[i].left   	  const dy = oldPositions[i].top - newPositions[i].top   	  el.style.transform = `translate(${dx}px, ${dy}px)`   	  el.style.transition = 'none'     })     // force reflow     void container.offsetHeight;      // 4. PLAY (play animation to new position)     items.forEach(el => {   	  el.style.transform = ''   	  el.style.transition = 'transform 300ms'     })   }) }   # Demo  Here is a demo with expandable/collapsable items and a sorting animation between different orderings.   ---  {{ addcomments }} "},
{title: "More gradient descent", content: "st](/2024/01/05/autograd), I didn't feel like I had fully addressed the overall meaning and motivation. The first thing that bothered me was that it felt like I was using the transpose of the derivative as an \"inverse\" map. But the transpose of the gradient obviously isn't an inverse. We're not able to map from the tangent space at $f(\\bm{x})$ to the tangent space at $\\bm{x}$. In the last post, I showed that the types work out (all the dual spaces line up), but that doesn't explain why the result is useful. I was missing the fact that we're not actually mapping some \"target\" value from the loss functions' tangent space into the input's tangent space. Instead, it turns out the transpose of the gradient is directly the value we want, and we're just calculating efficiently.  I'd like to thank Aidan Hennessey proof-reading and revealing the hidden secrets of manifolds. Now, I will prove that the transpose is indeed the \"right value\" and explain what the \"right value\" even means.  # What is the right thing  @@figure @@im-800  @@ Figure 1: On the left, we see the input space $V$ and the tangent space at $\\bm{x}$ which contains $\\bm{h}$ . On the right we have our output space $W$. Note that $Df(\\bm{x})\\bm{h}$ is in the tangent space at $f(\\bm{x})$. There is an interactive version of this plot [here](/2024/04/07/more-gradient-descent/#interactive_plot) @@  For gradient ascent (descent is in the opposite direction since we're on a plane), our goal is to maximize $f$ by finding the locally best direction. That is we want $\\delta$-length vector $\\bm{h}$ (in the tangent space of the input) such that it maximizes $f$:  $$ \\underset{\\|h\\|= \\delta}{\\operatorname*{argmax}\\,} f(\\bm{x} + \\bm{h}) $$ Where $\\delta$ is a small constant. The rest of this post is about why the argmax happens to be $\\bm{h} = Df(\\bm{x})^{\\intercal}.$  # Transpose maps  Let $f$ be the function that we care about $V\\to \\mathbb{R}$, from a vector space to the real numbers (we could actually choose any field we want, but most loss functions are of this form). Notice that the left side of this equation is a scalar  $$f(\\bm{x} + \\bm{f}) - f(\\bm{x}) = Df(\\bm{x})\\bm{h}.$$   Since $Df(\\bm{x})$ is turning a vector into a scalar, it must be a linear form. So, $Df(\\bm{x})$ is a member of the dual space of $V$ . Now, since the derivative is in the dual space, we can rewrite that expression as an inner product with a vector (the transpose of the derivative). To make that rigorous, given a basis $B = \\{\\bm{e^1}, \\dots ,\\bm{e^n}\\}$ of $V$, for all $\\bm{v} \\in V$, there are unique $\\alpha_{i}$ where $\\bm{v} = \\alpha_1\\bm{e^1} + \\dots  + \\alpha_n \\bm{e^n}$ (by definition of a basis). Then we can define basis elements of $V^*$ using  $$\\begin{align*} e_{i}: V \\to  \\mathbb{R}\\\\ \\bm{v} \\mapsto \\alpha_i \\end{align*}$$ $B^* = \\{e_1, \\dots , e_{n}\\}$ is a basis of $V^*$ which is the space of linear forms/covectors/dual space. Then given any element of the dual space, we can write it in coordinate form:  $$(\\sum_i \\alpha_i e_i)(\\bm{v}) = \\sum_i \\alpha_i e_i(\\bm{v}) = \\sum_i \\alpha_i v_i = \\langle \\alpha, \\bm{v} \\rangle$$ Let $$ \\begin{align*} \\phi: &V^* \\to  V\\\\ &e_{i} \\mapsto e^i \\end{align*} $$ be the transpose map. All $\\phi$ does is preserve the coefficients of the basis. It should be clear now, that $$ Df(\\bm{x})\\bm{h} = \\left\\langle  \\phi(Df(\\bm{x})), \\bm{h} \\right\\rangle = \\left\\langle  Df(\\bm{x})^{\\intercal}, \\bm{h} \\right\\rangle .$$  # Why is the “steepest ascent” = the transpose?  By unrolling the definition of the derivative, we have $$ \\begin{align*} \\underset{\\|h\\|= \\delta}{\\operatorname*{argmax}\\,} f(\\bm{x} + \\bm{h}) &\\approx \\underset{\\|h\\|= \\delta}{\\operatorname*{argmax}\\,} f(\\bm{x}) + Df(\\bm{x})\\bm{h}\\\\ &= \\underset{\\|h\\|= \\delta}{\\operatorname*{argmax}\\,} Df(\\bm{x})\\bm{h}\\\\ \\end{align*} $$ Let $\\bm{c}=Df(\\bm{x})^\\intercal$ for clarity. Then, using the dual space equation from earlier, we find: $$\\underset{\\|h\\|= \\delta}{\\operatorname*{argmax}\\,} Df(\\bm{x})\\bm{h}=\\underset{\\|h\\|= \\delta}{\\operatorname*{argmax}\\,} \\langle Df(\\bm{x})^\\intercal, \\bm{h}\\rangle=\\underset{\\|h\\|= \\delta}{\\operatorname*{argmax}\\,} \\langle \\bm{c}, \\bm{h}\\rangle$$  Now remember that $\\langle \\bm{a},\\bm{b} \\rangle \\leq \\|\\bm{a}\\|\\|\\bm{b}\\|$. And since $\\|\\bm{h}\\|=\\delta$, we have that $$ \\langle \\bm{c}, \\bm{h}\\rangle \\leq \\|\\bm{c}\\|\\delta $$  Consider $\\bm{h} = \\frac{\\delta}{\\|\\bm{c}\\|}\\bm{c}$. Then $$ \\langle \\bm{c}, \\bm{h}\\rangle = \\langle \\bm{c}, \\frac{\\delta}{\\|\\bm{c}\\|}\\bm{c}\\rangle = \\frac{\\delta}{\\|\\bm{c}\\|}\\|\\bm{c}\\|^2 = \\|\\bm{c}\\|\\delta $$  Thus, $\\bm{h} = \\frac{\\delta}{\\|\\bm{c}\\|}\\bm{c}$ achieves the maximum value. Thus, it must be the argmax. Therefore, $\\bm{h} \\propto Df(\\bm{x})^{\\intercal}$ gives the steepest ascent.  # Some real-world examples  Now that we believe the transpose of the gradient is actually the element of the tangent space that we want, let's try a nontrivial example. Consider this neural net:  @@figure @@im-most  @@ Figure 2: Consider a neural net with activation function $\\sigma$ and 2 hidden layers. $\\ell$ is the loss function, $x$ is the input vector, and $y$ is the expected output. Each $o_{i}$ is the output vector of a layer, and $W_{i}o_{i-1} + b_{i}$, the input to the activation function. @@  We can write this neural net as a function: $$ N(x; W_{i}, b_{i}) = o_{3} = \\sigma(W_{3}(\\sigma(W_{2}(\\sigma(W_{1}x + b_{1})) + b_{2})) + b_{3}) $$ and the loss as a function of the parameters: $$L(x, y; W_{i}, b_{i}) = \\ell(N(x; W_{i}, b_{i}), y)$$ Which is insane to expand out entirely. We're interested in the gradient of $W_{1}$ w.r.t. the loss. I'm going to define some helper functions ($h_{i}$) so our $f$ looks cleaner. So let's define  $$ \\begin{align*} h_{1}(o) &= l(o,y)\\\\ h_{2}(o) &= W_{3}o + b_{3}\\\\ h_{3}(o) &= W_{2}o + b_{2}\\\\ h_{4}(W) &= Wx + b_{1}\\\\ f(W) &= h_{1}(\\sigma(h_{2}(\\sigma(h_{3}(\\sigma(h_{4}(W))))))) .\\end{align*} $$ where the input $x$ and expected output $y$ and the other weights and biases are now treated as constants.  We should think of $f$  as the function that takes in a $W_{1}$ and outputs $\\ell(o_{3}, y)$, but I've written it in terms of a bunch of functions so that we can perform chain rule. Recall the chain rule (better explained in [my last post](/2024/01/05/autograd)): $$ \\begin{align*} Df(W) &= D(h_{1}\\circ \\sigma \\circ h_{2}\\circ \\sigma\\circ h_{3} \\circ \\sigma\\circ h_{4})(W)\\\\ &= Dh_{1}(o_{3})D\\sigma(a_{3})Dh_{2}(o_{2})D\\sigma(a_{2})Dh_{3}(o_{1})Dh_{4}(W)\\\\ \\end{align*} $$ Since $Dh_{1}(o_{3})$ is a 1 row matrix (since the output is a scalar), it is most efficient to do the multiplication left to right. This is backward mode autograd. And finally this brings us to the Jacobian-vector products of my last post. For example, let's say we've computed our row matrix $g^{\\intercal}$, so it looks like $$ \\underbrace{Dh_{1}(o_{3})D\\sigma(a_{3})Dh_{2}(o_{2})}_{g^{\\intercal}}D\\sigma(a_{2})Dh_{3}(o_{1})Dh_{4}(W) $$ It is potentially much faster to call $J^{\\intercal}\\sigma(a_{2}, g^{\\intercal})$ (it might even be $O(1)$, it depends on $\\sigma$) rather than do the matrix multiplication (which would be $O(n^{2})$ since we have to loop over all rows of the derivative, recall that $g^{\\intercal}$ is a 1 row matrix) $$ J^{\\intercal}\\sigma\\left( a_{2}, g^{\\intercal} \\right) = g^{\\intercal}D\\sigma(a_{2}) $$  # Wrapping Up  As we've seen, the transpose of the derivative happens to be the direction of steepest ascent. It is *not* an \"inverse map\" (the mistake I made in my last post). By using the Jacobian-vector products I introduced in my last post, we can calculate the gradient efficiently, which is critical for practical applications (like neural networks). To put it into practice, I presented an example derivation of the gradient, which you could use for gradient descent. # Interactive plot  The left side is the input space, and the right side is the output space. The arrow on the left is a small increment to the input. The arrow on the right is the approximate change in the output use the first order approximation and the red $\\times$ is the true function value.     Right now, it's a linear function: $f(\\bm{x}) = A\\bm{x} + \\bm{b}.$ Both the input and output are vectors. You can adjust the $h$ value and see how accurate the first order approximation (the arrow on the right graph). For the linear function it should be perfect, since it is it's own first order approximation.  Try the quadratic function (vector to scalar) and sin function (vector to vector). Note: I don't really have the time to animate multi dimensional graphs so we represent a scalar value $r$ as $(r, 0)$. You can find the code [here](https://github.com/jasoneveleth/d3-gradient-visualizer).   {{ addcomments }} "},
{title: "How does `scp` completion work in zsh?", content: "n you were `scp`-ing?   bash $ scp user@server.com:/home/user/do| downloads/    documents/   I have known for awhile that if you've set up ssh-keys, it would work. It'd been in the back of my mind to research, but I've never gotten around to it. That changed today.  My initial findings led me to [stack exchange](https://unix.stackexchange.com/questions/203931/whats-the-magic-that-allows-me-to-tab-complete-remote-files-as-i-type-an-scp-co). I'm 9 years late, but I figured I'd share some research I've done into this topic. You can ask zsh to give you the completion function for any command. So, to see for `scp`, you can use:  bash $ print $_comps[scp] _ssh   From this, we learn that we're looking for `_ssh`. I did some looking into zsh source and I found this file: [`Completion/Unix/Command/_ssh`](https://github.com/zsh-users/zsh/blob/master/Completion/Unix/Command/_ssh). After poking around in here, it seems like the relevant parts are here (when it's completing the file part of the command):  bash     file)       if compset -P 1 '[^./][^/]#:'; then         _remote_files -- ssh ${(kv)~opt_args[(I)-[FP1246]]/-P/-p} && ret=0       elif compset -P 1 '*@'; then         suf=( -S '' )         compset -S ':*' || suf=( -r: -S: )         _wanted hosts expl 'remote host name' _ssh_hosts $suf && ret=0       else         _alternative \\             'files:: _files' \\             'hosts:remote host name:_ssh_hosts -r: -S:' \\             'users:user:_ssh_users -qS@' && ret=0       fi       ;;   Looking at the first if condition, we see that if the argument doesn't start with `./` or `/`, then we use `_remote_files` command. Doing some more digging, we can find that there is a corresponding file (`Completion/Unix/Type/_remote_files`). You can find it on your computer with:   bash $ echo $functions_source[_remote_files] /usr/share/zsh/5.9/functions/_remote_files   Here's an excerpt of header comment:  > Needs key-based authentication with no passwords or a running ssh-agent to work.  So, we do need to have an ssh keys or some other passwordless way to ssh.   The file is just 104 lines long and pretty readable (if you're looked at zsh completion code before). The relevant line I didn't quite understand:   remfiles=(${(M)${(f)\"$(   _call_program files $cmd $cmd_args $host \\     command ls -d1FL -- \"$rempat\" 2>/dev/null )\"}%%[^/]#(|/)})   But after asking ChatGPT, the meat of the command is here: `$cmd $cmd_args $host command ls -d1FL -- \"$rempat\" 2>/dev/null` Here, `$cmd` is `ssh`, so it's running `ls -d1FL -- $rempat` on the remote machine.  I patched the file to print out the actual command that it runs (unfortunately Apple doesn't like this, so I had to get around SIP, see the last section). Here is the command in all it's glory:  bash ssh -o BatchMode=yes -a -x jason@machine.example.com command ls -d1FL -- /home/jason/\\*   This is the command that zsh is using to find files on remote machines.  # Breaking down the command  There's two parts to the command in the previous section. The part that gets executed on the remote machine: `command ls -d1FL -- /home/jason/\\*` and the part that gives options to ssh: `ssh -o BatchMode=yes -a -x jason@machine.example.com`  Breaking down the remote command, we use `ls` with  - `-d` to list directories as plain files (not their contents) - `-1` for one file per line - `-F` annotate directories with trailing `/`, and executables with `*`, etc - `-L` for follow symbolic links to their final destination - the `\\*` is to stop the glob from being expanded on the local machine - the `command` prefix is to ensure that we don't accidentally use an alias of `ls` on the remote machine  Breaking down the ssh options - `-o BatchMode=yes` user interaction such as password prompts and host key confirmation requests will be disabled - `-a` Disables forwarding of the authentication agent connection. (let's say 3 servers, `local -> A -> B`, this option means that you can't use your keys on `local` to connect to `B`. If you don't use `-a`, then when you are running commands on `A`, you could use your `local` keys to `ssh` from `A` to `B`) - `-x` Disables X11 forwarding  # Getting around SIP  I first tried to edit `/usr/share/zsh/5.9/functions/_remote_files` directly. This didn't work obviously because my user isn't root. I tried `sudo vim`, but that didn't work either. So (and I know this is bad) I tried `sudo sh` and then `vim`. This also didn't work. I realized that SIP was what was preventing me.  So, I unloaded the function and loaded my patch:  bash unfunction _remote_files autoload -U /tmp/_remote_files   {{ addcomments }} "},
{title: "A Rigorous (and Not-So-Rigorous) Look at JAX's Autograd", content: "nt](/2024/04/07/more-gradient-descent).) This post is correct but contains some misleading statements that I've labeled with an astricks*, and warnings at the beginning of each section. I wrote this post as I was learning differential geometry, thank you for joining me on this journey.}  # What this post is about  I've started writing this post from scratch several times as I've tried to get the presentation just right. I settled on this two goals: (1) if you have heard the terms \"[vector space](/404)\" and \"gradient\" thrown about before, you should be able to understand and write JAX primitives, and (2) if you're comfortable with vector spaces, dual spaces, and multivariable calculus, I will assuage any fears about rigor.  [JAX](https://jax.readthedocs.io/en/latest/) is a high-performance numerical computing framework for Python that can wrap a normal python function to calculate the derivative. We'll cover JAX-flavored reverse-mode automatic differentiation (autodiff), and focus on the 'differentiation' part. The 'automatic' part basically works by wrapping input `ndarray`'s in a custom class that keeps track of computation. That is out of the scope of this post, but you should check out the [Resources](/2024/01/05/autograd/#resources) section for more information (or my [110-line python implementation](https://github.com/jasoneveleth/autograd)). # Resources  I found plenty high-quality resources on autodiff. The first post I read was [this one](https://thenumb.at/Autodiff/), and I wrote down my takeaways [here](/404). [This post](https://jingnanshi.com/blog/autodiff.html) is also excellent. However, they cover mostly how to directly apply the chain rule. In contrast, the JAX-flavored way of doing things involved \"Jacobian-vector products\" and other foreign terms. I really liked the JAX approach of providing a higher-order `grad` function that can wrap any function you like. That approach seemed to have a structured procedure to applying the chain rule.  In my efforts, I stumbled upon many resources specifically related to JAX. One of the main contributors behind JAX made [a video](https://videolectures.net/deeplearning2017_johnson_automatic_differentiation/) about their framework (my notes on it are [here](/404)).  However, the most directly relevant information is the [autodiff cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobian-vector-products-jvps-aka-forward-mode-autodiff) for JAX (specifically the section I linked to) and [documentation](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html) on what you need to do to implement a primitive. Their approach is grounded in the literature, specifically [this paper from 2008](https://engineering.purdue.edu/~qobi/papers/toplas2008.pdf). And finally, I found a core JAX contributor's [phd thesis](https://dougalmaclaurin.com/phd-thesis.pdf) on the implementation of a precursor library (pages 13-19 and 48-52 are relevant).  # How does autodiff work?  First, we'll look at an example. Here is a simple neural net as a function of its input. I've colored each part of the function to correspond to its description. $$ f(\\bm{x}) := {\\color{red}  \\lvert\\lvert} {\\color{green}\\arctan(}{\\color{blue} W\\bm{x} + \\bm{b} }{\\color{green})} - {\\color{purple}\\bm{y} } {\\color{red} \\rvert\\rvert ^{2} } $$ This is a  . This is pretty standard (aside from the maybe uncommon choice of $\\arctan$). Notice that usually, you'd want to take the derivative with respect to the weights (so $W$ or $b$), but I'm keeping this example simpler. We will only take the derivative with respect to the input $\\bm{x}$ This way, we don't need to deal with multiple dimensions.  If all the computations for the autodiff of $f$  (evaluation and gradient accumulation) happened sequentially, it would look like the following. We will execute each operation of the neural net individually so that we can use these intermediate computations in our backward pass. Note: in `numpy`, the `.T` property takes the transpose of a 2D array. Don't worry about the `Jt_f(x, gt)` functions, they will be explained later.  python a = W @ x b = a + bias # I've replaced `b` from f(x) with `bias` c = np.arctan(b) d = c - y e = d**2 loss = np.sum(e) # begin backward mode gt = 1.0 e_gt = Jt_sum(loss, g_t)    # gt * np.ones((1, loss.shape[0])) d_gt = Jt_square(d, e_gt)   # e_gt * (2 * d.reshape(1,-1)) c_gt = Jt_subtract_const(c, d_gt) # d_gt b_gt = Jt_arctan(b, c_gt)   # c_gt * (1/(1 + b**2)).reshape(1, -1) a_gt = Jt_add_const(a, b_gt)# b_gt delta_xt = Jt_matmul(x, a_g)# a_gt @ W delta_x = delta_xt.T   There are several things to take away from this code snippet. We see that in backward-mode we are using the values from the original computation (`a`, `b`, `c`, etc.). I've put comments to inline the definitions of the `Jt` functions. And, at the end of the computation, we get a `delta_x`: the gradient of $f$ with respect to the input `x`.   # What are those `Jt_f(x, gt)` functions?  \\warn{I make three comments (labeled with an astricks*) that make it seem like the `Jt_func` is an inverse map of the derivative (i.e. a map from the tangent space of the output to the tangent space of the input). The function is just a efficient method of evaluated the transpose of the derivative on $g^{\\intercal}$, and since the derivative isn't guaranteed to be orthogonal, it is not an inverse map. It just so happens that $\\texttt{Jt\\_func}(\\bm{x}, \\bm{g}^{\\intercal})^{\\intercal}$ is the steepest ascent when $\\bm{g}$ is the value in our backwards mode differentiation.}  I'm borrowing the transpose vector Jacobian product (`Jt`) from [JAX documentation](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobian-vector-products-jvps-aka-forward-mode-autodiff). I keep the notation the same so if you read the JAX documentation, their notation will be familiar. **The next few paragraphs are meant to click later in the post; please don't give up just because it doesn't quite make sense now.**  When looking at a function call like `Jt_f(x, gt)`, the $\\bm{x}$ tells us where in the input space of some $f:\\mathbb{R}^{n}\\to \\mathbb{R}^{m}$ we are, and, morally[^1] , $\\bm{g}^{\\intercal}$  is a direction in the output space of that $f$. `Jt_func` turns that direction in the output space, to a direction of the gradient in the input space.* Here's a picture:  @@figure @@im-800  @@ Figure 1: On the right we have our output space. I named a vector in this space $\\bm{g}^{\\intercal}$ (later I will explain why I use the transpose notation). For now, we will use it as a row vector. On the left, we see the output of `Jt_f(x, gt)`, which is an increment to the input. @@  You can see the direction $\\bm{g}^{\\intercal}$ in the output being transformed into the input space.* This is how we propagate the gradient through `func`. If you have the gradient of $f$ at $\\bm{x}$, you can compute $$ \\texttt{Jt\\_func}(\\bm{x}, \\bm{g}^{\\intercal}) = \\bm{g}^{\\intercal} \\left(\\left.\\frac{d}{d \\bm{y} }f(\\bm{y})\\right|_{\\bm{y}=\\bm{x}}\\right) $$ Where $\\left.\\frac{d}{d\\bm{y} }f(\\bm{y})\\right|_{\\bm{y}=\\bm{x}}$ is the Jacobian matrix of $f$ at $\\bm{x}$. [Leibniz's notation](https://en.wikipedia.org/wiki/Leibniz%27s_notation) is insane, so we're going to adopt the notation from [Spivak](http://www.strangebeautiful.com/other-texts/spivak-calc-manifolds.pdf), and [other](http://ceres-solver.org/spivak_notation.html) [people](http://www.vendian.org/mncharity/dir3/dxdoc/) [agree](https://mitpress.mit.edu/9780262019347/functional-differential-geometry/). Let me clarify the notation a little bit more. In math, we often confuse functions and expressions, so if $h(x) := 2x + 5$, we would talk about the function $h(x)$, when we should be talking about the function $h$. Normally the derivative operator applies to expressions, so we write $\\frac{d}{dx}(2x + 5)$ or $\\frac{d}{dx}h(x)$ which means \"expand $h$ into an expression and then differentiate the expression\". In the notation I’m using, the derivative operator applies to functions, we write $\\frac{d}{dx}h$, which is a new function with same input type. We abbreviate $\\frac{d}{dx}$ as $D$. Thus, let $$(Df)(\\bm{x}) = Df(\\bm{x}) = \\left.\\frac{d}{d\\bm{y} }f(\\bm{y})\\right|_{\\bm{y}=\\bm{x}}$$ be the Jacobian matrix. Note that I'll leave off the parens and assume that $D$ always applies to it's function before we apply the function to $x$. Finally, let's rewrite the equation: $$ \\texttt{Jt\\_func}(\\bm{x}, \\bm{g}^{\\intercal}) := \\bm{g}^{\\intercal}Df(\\bm{x}). $$ Much better. Note that $Df(\\bm{x})$ is a matrix, and $\\bm{g}^{\\intercal}$ is any column vector (we'll talk about what $\\bm{g}$ really is [later](/2024/01/05/autograd/#what_is_going_on_bringing_back_rigor)). Now, why does composing `Jt_func` work? We'll hold off on that for a bit (unless you can't wait, in which case it's [here](/2024/01/05/autograd/#what_is_going_on_bringing_back_rigor)). If you're confused right now, that's entirely fair. We'll go through many examples, and then I'll explain the underlying theory. I've just said this stuff so it will click later on.  The way we figure out how to implement these `Jt` functions is we evaluate $\\bm{g}^{\\intercal}Df(\\bm{x})$ (which you should think about as taking the direction $\\bm{g}^{\\intercal}$ from output space to a direction in the input space)* for a given $f$. Let's start with the functions I introduced in the simple neural net at the beginning of this article.   ## Implementing `Jt_subtract_const`  We will start with `subtract_const`. $f(\\bm{x}) := \\bm{x} - \\bm{b}$ with $\\bm{b}$ a constant. We can calculate the derivative by plucking it out of the taylor series: $$ f(\\bm{x}+ \\Delta \\bm{x}) = f(\\bm{x}) + Df(\\bm{x})\\Delta \\bm{x} + O(\\Delta \\bm{x}^2). $$ We collect terms, and the coefficient of the first-order term is the derivative. We can do this for our function $f$ : $$ \\begin{align*} f(\\bm{x}+\\Delta \\bm{x}) &= (\\bm{x} + \\Delta \\bm{x}) - \\bm{b}\\\\ &= \\bm{x} - \\bm{b} + \\Delta \\bm{x} \\\\ &= f(\\bm{x}) + I\\Delta \\bm{x} \\\\ \\end{align*} $$ Thus, $Df(\\bm{x}) = I$ the identity matrix. So, If we want to calculate `Jt_subtract_const` we need to figure out what $\\bm{g}^{\\intercal}I$ is, which is simply $\\bm{g}^{\\intercal}$. Thus, python def Jt_subtract_const(x, gt):     \"\"\"x: ndarray shape(n,)        gt: ndarray shape(1,n)\"\"\"     return gt   Let's do a harder one.  ## Implementing `Jt_square`  Now $f(\\bm{x}) := \\bm{x}^{2}$ element-wise. Remember, to find the derivative, we take $$ \\begin{align*} f(\\bm{x} + \\Delta\\bm{x}) &=(\\bm{x} + \\Delta\\bm{x})^{2}\\\\ &=(\\bm{x} + \\Delta\\bm{x})\\odot(\\bm{x} + \\Delta\\bm{x})\\\\ &=\\bm{x}^{2}+2\\bm{x} \\odot \\Delta\\bm{x} + (\\Delta\\bm{x})^{2}\\\\ &=f(\\bm{x})+2\\bm{x} \\odot\\Delta\\bm{x} + O((\\Delta\\bm{x})^{2}).\\\\ \\end{align*} $$ With $\\odot$ as element-wise product. Thus, if we name each component of the vector non bold: $$ \\bm{x} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots\\\\x_{n} \\end{pmatrix}, $$ then  $$ Df(\\bm{x}) = \\begin{bmatrix} 2x_{1}& 0 & \\dots& 0 \\\\ 0& \\ddots & & \\\\ & & \\ddots & 0\\\\ & & 0& 2x_{n}\\\\ \\end{bmatrix}. $$  Since this matrix does the same thing as multiplying element-wise by $2\\bm{x}$ . Multiplying this with $\\bm{g}^{\\intercal}$ gives $[2x_{1}g_{1} \\dots 2x_{n}g_{n}]$. So, python def Jt_square(x, gt):     \"\"\"x: ndarray shape(n,)        gt: ndarray shape(1,n)\"\"\"     return gt * (2 * x.reshape(1, -1))   ## Implementing `Jt_arctan`  Now $f(\\bm{x}) := \\arctan(\\bm{x})$ element-wise. This one is much trickier. There's probably a way to implement `Jt_arctan` using the Taylor series, but there is an easier way in this scenario. To understand it, let's name the components of the output: $$f(\\bm{x})=\\begin{pmatrix} f_{1}(\\bm{x})\\\\ \\vdots\\\\ f_{n}(\\bm{x}) \\end{pmatrix}=\\begin{pmatrix} \\arctan(x_{1})\\\\ \\vdots\\\\ \\arctan(x_{n}) \\end{pmatrix}.$$ When we first learn multivariable calculus, you're probably taught that the definition of the Jacobian is the matrix of partial derivatives, i.e. $$ Df(\\bm{x}) = \\begin{bmatrix} \\partial_{1}f_{1}(\\bm{x})& \\partial_{2}f_{1}(\\bm{x}) & \\dots& \\partial_{n}f_{1}(\\bm{x}) \\\\ \\partial_{1}f_{2}(\\bm{x})& \\ddots & & \\\\ \\dots& & \\ddots & \\partial_{n}f_{n-1}(\\bm{x})\\\\ & & \\partial_{n-1}f_{n}(\\bm{x})& \\partial_{n}f_{n}(\\bm{x})\\\\ \\end{bmatrix}. $$  We're going to exploit the fact that the Jacobian happens to equal this matrix of partial derivatives since we know how to evaluate the scalar $\\arctan$. Because our function $f$ is element-wise, only the diagonal will have values on it. That's because $x_{m}$ has no effect on $f_{k}(\\bm{x}) = \\arctan(x_{k})$ unless $k=m$ (which would put it on the diagonal). This is true in general: element-wise operations yield a diagonal Jacobian. In our scenario, since $D(\\arctan)(x)=1/(1 + x^{2})$, this gives us $$ Df(\\bm{x}) = \\begin{bmatrix} 1 /(1 + x_{1}^{2})& 0 & \\dots& 0 \\\\ 0& \\ddots & & \\\\ & & \\ddots & 0\\\\ & & 0& 1 / (1 + x_{n}^{2})\\\\ \\end{bmatrix} $$  Putting it into code we get: python def Jt_arctan(x, gt):     \"\"\"x: ndarray shape(n,)        gt: ndarray shape(1,n)\"\"\"     return gt * (1/(1 + x**2)).reshape(1, -1)  # What is going on (bringing back rigor)  \\warn{I claim \"$\\bm{g}$ isn't special\" when it is. Otherwise the $Df(x)^{\\intercal}\\bm{g}$ is just a random value in a vector space with the same dimension as the tangent space in the input. Also, while it is correct, the side quest about the dual space just makes the article a little less clear.}  You might be thinking: I'm familiar with multivariable calculus, I love a good dual space, but this all feels unjustified. To fix that, we'll look again at what a derivative really is. Given a function $f:V\\to W$ between vector spaces, the derivative is *the* linear map that takes incremental inputs of $f$ and will output incremental changes in the value of $f$. It is often represented as a matrix, but to clarify the types, we'll pretend it's just a linear function. That is, $Df: V \\to (V \\to W)$, and $Df(\\bm{x}): V\\to W$. That is, we're currying the function when we evaluate it at $\\bm{x}$.  It has a nice property that $$ f(\\bm{x}) + Df(\\bm{x})(\\bm{h})\\approx f(\\bm{x}+\\bm{h}). $$We're using slightly different notation since we're treating $Df(\\bm{x})$ as a function, but remember, the application of it as a function is just left multiplication with the matrix. With our new understanding that the derivative is a linear map (and it just happens to be a matrix). Let's look again at this figure  @@figure @@im-800  @@ Figure 2: On the left, we see the input space $V$ and the tangent space at $\\bm{x}$ which contains $\\bm{h}$ . On the right we have our output space $W$. Note that $Df(\\bm{x})\\bm{h}$ is in the tangent space at $f(\\bm{x})$  @@  There are several things to notice. First, I've drawn the dashed axes to represent the domain and codomain of $Df(\\bm{x})$. Now, since we're doing reverse mode, we need to go the other direction: take incremental changes in output and turn them into incremental changes to input. Luckily, we can take the transpose of a linear map: $$ \\begin{align*} Df(\\bm{x})^{\\intercal}:W^*\\to V^*\\\\ Df(\\bm{x})^{\\intercal}(\\bm{g})=\\bm{g}\\circ Df(\\bm{x}) \\end{align*} $$ Where $\\bm{g}\\in W^*$. It is important to realize $\\bm{g}$ isn't special.* It can be any element of $W^{*}$ . We're now dealing with the dual space. Elements are often referred to as covectors. They are a linear map from their \"normal\" space to the underlying field. Thus, $\\bm{g}:W\\to \\mathbb{R}$. This means the types the expression make sense: $\\bm{g}\\circ Df(\\bm{x}):V\\to \\mathbb{R}$, or $\\bm{g}\\circ Df(\\bm{x}):V^*$ . Often people think of the dual space as linear functions that are \"taking the inner product with a vector\". So $$ \\bm{g}(\\bm{x}) = \\langle \\bm{g},\\bm{x} \\rangle. $$  This is especially useful because we can then represent $\\bm{g}$ as a row vector, which, when combined with a column vector from the nondual space, gives the inner product. This is the reason I've been so careful with keeping the `gt` of shape `(1,n)` because these $\\bm{g}$s are members of the dual space and are actually row vectors. From now on, I'll use $\\bm{g}^{\\intercal}$ as a reminder that it's a row vector.   Just to check that using the covectors as row vectors will work, let's examine our vector-Jacobian product $$ (\\bm{g}\\circ Df(\\bm{x}))(\\bm{v})=(\\bm{g}^{\\intercal} Df(\\bm{x}))\\bm{v}=\\bm{g}^{\\intercal} Df(\\bm{x})\\bm{v} $$  Thus, with our newfound correspondence, let's evaluate the vector-Jacobian product (and employ chain rule). Let $f = C \\circ B \\circ A$, then $$ \\begin{align*} Df(\\bm{x})^{\\intercal}(\\bm{g})&=\\bm{g}^{\\intercal} Df(\\bm{x})\\\\ &=\\bm{g}^{\\intercal} \\,((DC \\circ B \\circ A)(\\bm{x})\\,\\,(DB \\circ A)(\\bm{x})\\,  DA(\\bm{x}))\\\\ &=\\bm{g}^{\\intercal}\\, DC((B \\circ A)(\\bm{x}))\\,DB (A(\\bm{x})) \\, DA(\\bm{x})\\\\ \\end{align*} $$ Recall our definition from earlier: $$ \\texttt{Jt\\_f}(\\bm{x}, \\bm{g}^{\\intercal}) = \\bm{g}^{\\intercal}Df(\\bm{x}). $$ Then $$ \\begin{align*} &=\\bm{g}^{\\intercal} DC((B \\circ A)(\\bm{x}))\\,\\,DB (A(\\bm{x}))\\,  DA(\\bm{x})\\\\ &=\\texttt{Jt\\_C}((B\\circ A)(\\bm{x}), \\bm{g}^{\\intercal})\\,DB (A(\\bm{x}))\\,  DA(\\bm{x})\\\\ \\end{align*} $$ What we should notice here is that $\\texttt{Jt\\_C}((B\\circ A)(\\bm{x}), \\bm{g}^{\\intercal}) = \\bm{g}^{\\intercal}DC((B\\circ A)(\\bm{x}))$ is an element of the dual space of the output of $B$. Thus, we can apply this strategy again, where we incorporate an element into a call to a `Jt` function:  $$ \\begin{align*} &=\\texttt{Jt\\_C}((B\\circ A)(\\bm{x}), \\bm{g}^{\\intercal})\\,DB (A(\\bm{x}))\\,  DA(\\bm{x})\\\\ &= \\texttt{Jt\\_B}(A(\\bm{x}),\\,\\, \\texttt{Jt\\_C}((B\\circ A)(\\bm{x}), \\bm{g}^{\\intercal})) \\,\\,  DA(\\bm{x})\\\\ &= \\texttt{Jt\\_A}(\\bm{x},\\,\\, \\texttt{Jt\\_B}(A(\\bm{x}), \\texttt{Jt\\_C}((B\\circ A)(\\bm{x}), \\bm{g}^{\\intercal})))\\\\ \\end{align*} $$  This is very busy notation, so to look at it better, let $\\bm{a}=A(\\bm{x}), \\bm{b}=B(A(\\bm{x}))$ then the final expression becomes $$ \\texttt{Jt\\_A}(\\bm{x},\\, \\texttt{Jt\\_B}(\\bm{a},\\, \\texttt{Jt\\_C}(\\bm{b},\\, \\bm{g}^{\\intercal}))). $$ This should hopefully justify the program I provided at the beginning of the post. Just by knowing the `Jt` functions of the primitives, we end up with the ability to convert $\\bm{g}^{\\intercal}$ into a covector of the input space of $f$. Now let's reflect on what this means. If we choose $\\bm{g}^{\\intercal}=\\begin{bmatrix} 1 & 0& \\dots&0 \\end{bmatrix}$. Then we get back a row of the derivative: $$ \\begin{align*} \\bm{g}^{\\intercal}Df(\\bm{x}) &=\\begin{bmatrix} 1 & 0& \\dots&0 \\end{bmatrix}\\, Df(\\bm{x})\\\\ &=\\begin{bmatrix} \\partial_{1}f(\\bm{x}) & \\partial_{2}f(\\bm{x})& \\dots&\\partial_{n}f(\\bm{x}) \\end{bmatrix} \\end{align*} $$ (borrowing notation from [Sussman](https://mitpress.mit.edu/9780262019347/functional-differential-geometry/), $\\partial_{1}f$ is the partial derivative with respect to the first argument).  This is great because almost all loss functions in ML are scalar, so the Jacobian just has one row, and thus, we can efficiently compute the gradient for our descent.  You may be thinking, hold on a minute, the gradient is actually a linear form (in the case of a scalar loss function): we can't add that to $\\bm{x}$, and you're right. We should think of the gradient as something that can turn small changes of $\\bm{x}$ into small changes in $f(\\bm{x})$. When we're doing gradient descent, we want a small change in $\\bm{x}$ that will maximally decrease $f(\\bm{x})$ for a given step size. The small change in $\\bm{x}$ that will cause the most increase is in fact the transpose of the gradient (this makes sense since the dot product is maximal when the vectors are parallel). So, it is okay to use the gradient for gradient descent after all.  It is important to note that the components of the Jacobian depend on the basis of the vector spaces of the domain and codomain.  # Wrapping up  I hope this post helped you understand JAX backward-mode automatic differentiation. I wrote an [implementation](https://github.com/jasoneveleth/autograd) that automatically tracks the computations, so you don't have to write out the `Jt_f()` functions explicitly. This is one of my most technical posts. Let me know if any part of it needs better or more in depth explanation.  I wrote a follow-up post [here](/2024/04/07/more-gradient-descent), which should clarify what gradient descent actually is and why these Jacobian vector products help.  [^1]:  morally in [this](https://eugeniacheng.com/wp-content/uploads/2017/02/cheng-morality.pdf) sense  {{ addcomments }} "},
{title: "A Linear Algebra Perspective on High School Chemistry", content: " up, and I realized that high school stoichiometry seemed like a algorithm I should recognize.  In stoichiometry, you're given an equation like this: $$x_1C_6H_{12}O_6 + x_2O_2 = x_3H_2O + x_4CO_2.$$ We're trying to determine what the coefficients need to be for the equation to be balanced. From what I remember from high school, you just think really hard until you figure out what they need to be, and that's how you determine the coefficients.  Taking a more systematic approach, we can move everything over to the other side, giving $$x_1C_6H_{12}O_6 + x_2O_2 - x_3H_2O - x_4CO_2 = 0$$  We can consider the balance for each element as an equation, and our coefficients as unknowns $$ \\begin{align*} C & &6x_1 + 0x_2 - 0x_3 - 1x_4 = 0\\\\ H & &12x_1 + 0x_2 - 2x_3 - 0x_4 = 0\\\\ O & &6x_1 + 2x_2 - 1x_3 - 2x_4 = 0\\\\ \\end{align*} $$  Now it's just a problem of Gaussian elimination. This is a homogeneous linear system:  $$ \\begin{bmatrix} 6 & 0 & 0 & -1\\\\ 12 & 0 & -2 & 0\\\\ 6 & 2 & -1 & -2\\\\ \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ \\end{bmatrix} = \\begin{bmatrix} 0\\\\0\\\\0\\\\ \\end{bmatrix}. $$ This is so exciting, because now you can use your favorite linear solver on this equation to determine the balance for the original chemical reaction.  {{ addcomments }} "},
{title: "Professor Chan's Band Coloring", content: "save her musical camp director hours of work. Keep in mind that this was 4 years ago, so the details are a little shaky. Her musical camp director had dozens of students, and each of them was in at least 1 band. The problem he was trying to solve was assigning student bands to time slots for practicing. The issue was that students could be in multiple bands but only in one timeslot at a time.  Let's make this problem concrete. Let's say you have 9 students numbered $1$ through $9$. Let's say you have 3 time slots 8am-9am, 9am-10am, and 10am-11am. The bands are labeled with letters $a=\\{2,5,7,9\\},b=\\{6,7\\},c=\\{1,2,3\\},d=\\{3\\},e=\\{1,3,8,9\\},f=\\{4,7,8\\}$. This example isn't too hard to schedule by hand, but it would be tricky if you had many more students and many more bands. A solution to this scheduling would look like: $a,d$ to 8am, $b,e$ to 9am, and $c,f$ to 10am. You can check that none of the students overlap in the same time slot.  Now, in comes the graph theory. We can turn this problem into a graph by letting each band be a vertex and each timeslot be a color. Let's say 8am is red, 9am is green, and 10am is blue. Here is the resulting graph:  @@im-half  @@  I've drawn edges between bands that share students. The restriction of a valid coloring means that a student never has to be in the same time slot for two different bands.  This was a very elegant solution to the problem and she said that her code still runs and schedules the bands practice times to this day.  {{ addcomments }} "},
{title: "Asymptotic Analysis of Dijkstras in Haskell", content: "ces from Providence to all the other cities. We can turn that map into a network of cities and distances between them. In math and computer science, we call this a graph (and we refer to the cities as vertices or nodes). Our goal in this post is to using functional programming to find the shortest distances from Providence using Dijkstra's algorithm. It's going to get CS-heavy in the rest of the post so buckle up.  @@im-quarter  @@  TLDR: Even though Haskell's `Map` and `Set` are $O(\\log N)$[^1], I was able still get Dijkstra's in $O((V + E)\\log V)$. I used the [PSQueue](https://hackage.haskell.org/package/PSQueue-1.2.0/docs/Data-PSQueue.html#t:Binding) library for my priority queue.  Let's first recall the structure of Dijkstra's algorithm. I've included pseudo code below with asymptotic analysis for each line. The idea is we have a frontier (`Q`) that keeps track of what we think the shortest path is, and we update it as we remove the closest city (and add it's neighbors). Dijkstra's overall runtime is $O((V + E) \\log V)$ or $O(V \\log V + E)$ using Fibonacci heaps.  We need a visited set so we don't re-explore cities that we've already gone to.   dijkstras(s,G)      Q = Heap()                                          O(1)      visited = set()                                     O(1)      weights = map()                                     O(1)       Q.put(s, 0)      while !Q.empty                                      O(V)              n,w = Q.pop_min()                           O(V * log V)              weights[n] = w                              O(V * 1)              visited.add(n)                              O(V * 1)              for (w_m, m) in neighbors of n in G         O(E)                      if m visited: skip                  O(E * 1)                      Q.insert_or_decrease(m, w + w_m)    O(E * log V) O(E)   So now, the question is, can this be done in the same time complexity, and the answer is yes[^1]. I wrote two haskell functions, `dijkstras` which does the outside loop of taking a vertex off the queue, updating the frontier, the map of weights that we'll return, and the visited set. To update the frontier, we need to loop through all the edges connected to the vertex we just popped off. I made a function specifically for that which checks for containment in the visited set, and will either insert or decrease the priority of the node. Here's the code, with asymptotic analysis on the side. Anything with an exclamation point is asymptotically faster in the imperative version, but the overall runtime turns out to be the same.  haskell type Node = String -- adjacency list type Graph = Map.Map Node [(Float, Node)]  dijkstras :: Graph -> Node -> Map.Map Node Float dijkstras graph start = dijkstras_helper Set.empty (PSQ.singleton start 0) Map.empty   where     dijkstras_helper visited frontier weights =        case PSQ.minView frontier of                         -- O(V)         Nothing -> weights                                 -- O(V * 1 / V)         Just (binding, frontier') -> let                   -- O(V * 1)             node = PSQ.key binding                         -- O(V * 1)             w = PSQ.prio binding                           -- O(V * 1)             edges = fromJust $ Map.lookup node graph       -- O(V * log V)!             visited' = Set.insert node visited             -- O(V * log V)!             weights' = Map.insert node w weights           -- O(V * log V)!             frontier'' = foldr (update_keys visited' w) frontier' edges -- O(E * log V)             in             dijkstras_helper visited' frontier'' weights'   -- O(V * 1)  -- acts as the inner loop of dijkstras, goes through the adjacent nodes and updates the frontier update_keys :: Set.Set Node -> Float -> (Float, Node) -> PSQ.PSQ Node Float -> PSQ.PSQ Node Float update_keys visited w_0 (w, node) acc =    if Set.member node visited                               -- O(log V)!   then acc                                                 -- O(1)   else      case PSQ.lookup node acc of                            -- O(log V)!       Nothing -> PSQ.insert node (w + w_0) acc             -- O(log V)       Just p -> PSQ.adjust (\\_ -> min p (w + w_0)) node acc-- O(log V)  the_us :: Graph the_us = Map.fromList [     (\"Prov\", [(2.0, \"NY\"), (1.0, \"Boston\"), (3.0, \"Chester\")]),      (\"NY\", [(2.7, \"Boston\"), (2.0, \"Prov\"), (5.0, \"DC\")]),      (\"Boston\", [(1.0, \"Prov\"), (2.5, \"Chester\")]),     (\"DC\", [(5.0, \"NY\")]),     (\"Chester\", [(3.0, \"Prov\"), (2.5, \"Boston\")])     ]  main :: IO () main = do      putStrLn \"Map of the US:\"     putStrLn $ show $ dijkstras the_us \"Prov\"   [^1]: Here are the docs with asymptotic runtimes: [set](https://hackage.haskell.org/package/containers-0.7/docs/Data-Set.html) and [map](https://hackage.haskell.org/package/containers-0.4.0.0/docs/Data-Map.html)    {{ addcomments }} "},
{title: "Wave equation simulation", content: " frequency in the center of her room. You're allowed to put speakers (which don't take up space) wherever you want in the room. The question is, what's the best place to put them and what should they play?  To be fair, this is not the question that I'll be answering in this post, but it was the inspiration. We first simplify the problem into 2D so your enemy is now vibrating a surface with constant frequency. To simulate this problem, we need to solve the wave equation, which I do in this post.  At first, I tried to figure out the finite difference equations, but I got stuck. Then, I looked it up and I found [this wonderful blog post](https://beltoforion.de/en/recreational_mathematics/2d-wave-equation.php). I've rederived the equations to make sure I believed it and I wrote the code in Julia, which you can find it on [GitHub](). My first attempt at figuring out the finite differences is in the Python notebook, and the final code is in `simulation.jl`.  # In action   # The rederivation  See [calculus notation](/404) for notation. The wave equation is  $$D_{tt}u = c^2 ( D_{xx}u + D_{yy}u)$$  First, we figure out what the finite difference wave equation looks like. Since we have a second time derivative, we need to keep 2 past states to calculate the difference in the first derivative. Look at this picture ($u_{0,i,j}$ is the newest state)  @@im-most ![Mode of action of the discretized version of the differential operator for the solution of the wave equation](https://beltoforion.de/en/recreational_mathematics/images/diskretisierung_wellengleichung.webp) @@  Let's do the time derivative first. $$D_tu_{1,i,j} = \\frac{\\Delta_t u_{1,i,j}}{dt} = \\frac{u_{1,i,j} - u_{2, i, j}}{dt}$$ So, $$D_{tt}u_{1,i,j} = \\frac{\\Delta_t D_t u_{1,i,j}}{dt}=\\frac{\\frac{u_{0,i,j} - u_{1, i, j}}{dt}-\\frac{u_{1,i,j} - u_{2, i, j}}{dt}}{dt} = \\frac{u_{0,i,j}-2u_{1,i,j}+u_{2,i,j}}{(dt)^2}$$ Next, we do space derivatives (ignoring $c^2$ for now), $$ \\begin{align*} D_{xx}u_{1,i,j} &+ D_{yy}u_{1,i,j} = \\frac{\\Delta_x D_x u_{1,i,j}}{dx} + \\frac{\\Delta_y D_y u_{1,i,j}}{dy}\\\\ &= \\frac{u_{1,i-1,j} - 2u_{1,i,j} + u_{1,i+1,j}}{(dx)^2} + \\frac{u_{1,i,j-1} - 2u_{1,i,j} + u_{1,i,j+1}}{(dy)^2} \\end{align*}$$  Returning to the wave equation, $$ \\begin{align*} D_{tt}u &= c^2 ( D_{xx}u + D_{yy}u)\\\\ \\frac{u_{0,i,j}-2u_{1,i,j}+u_{2,i,j}}{(dt)^2} &= c^2\\left(\\frac{u_{1,i-1,j} - 2u_{1,i,j} + u_{1,i+1,j}}{(dx)^2} + \\frac{u_{1,i,j-1} - 2u_{1,i,j} + u_{1,i,j+1}}{(dy)^2}\\right)\\\\ u_{0,i,j} &= 2u_{1,i,j} - u_{2,i,j} +c^2 (dt)^2\\left( \\frac{u_{1,i-1,j} - 2u_{1,i,j} + u_{1,i+1,j}}{(dx)^2} + \\frac{u_{1,i,j-1} - 2u_{1,i,j} + u_{1,i,j+1}}{(dy)^2}\\right) \\end{align*} $$  If we assume $dx = dy$ and $\\alpha = \\frac{c^2 (dt)^2}{(dx)^2}$ then $$u_{0,i,j} = 2u_{1,i,j} - u_{2,i,j} +\\alpha\\left( u_{1,i-1,j}+u_{1,i,j-1} - 4u_{1,i,j} + u_{1,i+1,j} + u_{1,i,j+1}\\right)$$ There are no $u_{0,\\_,\\_}$ on the RHS. Thus, every value in the newest level can be calculated by the ones in previous levels.  We deal with the boundary by only calculating `u[0,1:lx,1:ly]`, and leaving the rest 0. So shifts are calculated like so: `u[1,0:lx-2,1:ly]`.  Note: this is actually [Verlet integration](https://en.wikipedia.org/wiki/Verlet_integration) (a trick for solving for the next state in terms of the finite differences and the second derivatives).  ## Implementation details  We can play a frequency by resetting the value of a $u_{0,i,j}$ after the step to the value of a sine wave at the time. We can make a water droplet by setting part of the $u_{0,i,j}$ layer to a negative gaussian at a certain time step. This is achieved with `put_drop!()` and `play_noise!()` functions.  {{ addcomments }} "},
{title: "hashfs", content: "ere syncing correctly between two laptops. I'd been thinking about git a lot, and I realized recursive hashing was the way to go.  The problem I was trying to solve was making comparisons between two directory trees easy. The solution I came up with was to use the sha256 digest of file contents to represent files. This prevents us from having to look at the file contents, and the comparison between two files is easy since the digests are just numbers. We can use the hash of all the files in the directory to represent a directory. So, to compare if two directories are identical, we just need to look at the digests that represent them. This solves the problem because now it is easy to see which parts of a directory tree are the same.  It remains to figure out how to display this information. I've been working with HTML a lot recently working on this blog, and I realized a `<details>` element is the perfect fit. It hides the contents unless you click on it, so you don't need to look at the files which have the same hashes. And, we can nest `<details>` elements to create a tree that mirrors the directory structure.  I choose to write this up in Rust since I love Rust. The code can be found on [github](https://github.com/jasoneveleth/hashfs).  This utility ended up not being useful since iCloud automatically frees up space by replacing files with a `file.ext.icloud` file, which ruins the whole hashing system. Regardless, I am very proud of it, and I think it could be useful to others.  # In action  For a directory tree like:  b |- a.txt |- b.txt |- c.txt |- d.txt |- e.txt |- f.txt |- g.txt |- j.txt  It would produce this (hashes have been elided for space):  # Update 8/9/2023  One of my friends let me know that I have implemented a [Merkle tree](https://en.wikipedia.org/wiki/Merkle_tree). Merkle trees are trees whose leaves store the hash of their contents, and whose interior nodes store the hashes of their children.   They are often used in cryptocurrencies because you can prove the inclusion of a leaf in the tree by just providing the child nodes corresponding to the path from that leaf to the root. This works in log time because the tree is $log(N)$ tall.  {{ addcomments }} "},
{title: "Colorspace in glium", content: "). I decided to use Rust since it is the best programming language ever conceived by man. The obvious choice for an OpenGL project in Rust is to use [`glium`](https://github.com/glium/glium/). However, I found their API a little confusing. I thought it would be useful to have a guide on how to deal with colorspaces in `glium` and OpenGL. This is that guide. **Note: This is my first foray into graphics, if anything is wrong, or hints at a misunderstanding on my part, please let me know.**  # Concepts involved  - `GL_FRAMEBUFFER_SRGB`: A global parameter, associated with an OpenGL context. It comes up so much, I will refer to it as `GFS`. How it affects gamma correction is explained in the [Summary of literature](/2023/07/31/understanding-GLFRAMEBUFFERSRGB-in-glium/#summary_of_literature). - Texture: a place to store image data/raster graphics (pixel values)     - can come in 2 variants: regular and sRGB     - can be used as a render target (in which case the texture will be attached on an FBO, mentioned below) or as a source of image data - Framebuffer: a collection of buffers used as a destination for rendering. Can be the Default Framebuffer or a user-created Framebuffer Object (FBO).  # \"help my colors are broken\" flowchart (for the current glium way)  Many of the issues people have encountered would be solved by this flow chart, so I've included it before the rest of my learnings.  @@im-most ![](/assets/help-colors.jpg) @@  # Summary of literature  Normally, color values in images are assumed to be in linear color space. If an image is in sRGB color format (ie. `SrgbTexture2d` vs `Texture2d`), values are assumed to be stored in sRGB.  \"When fetching from sRGB images in shaders, values are converted from sRGB into linear colorspace.\" Thus, the shader only sees linear values.  If OpenGL needs to filter sampled values, the implementation is allowed to filter before or after the sRGB conversion (and since filtering is linear, you want it to happen in a linear space (ie. linear RGB)).  [source](https://www.khronos.org/opengl/wiki/Image_Format)  \"Normally, sRGB images perform color correction, such that texture reads from them will always convert to linear [RGB]\".  When writing from a fragment shader, we need to know how to interpret the values written by the shader. If the output buffer is linear RGB, we assume the answer is linear. But if we're writing to an sRGB image, maybe we want to write linear or sRGB. For this, we need a global toggle since the gamma correction partly depends on the output buffer. This is the `GFS` parameter.  When `GFS` is: - disabled: assume the output buffer is the same color space as input buffer (we assume the user knows what they're doing) - enabled: if destination is[^2] 	- sRGB: assume shader output is linear, so it converts linear RGB to sRGB 	- non-sRGB: no conversion  [^2]: you can check this with `glGetFramebufferAttachmentParameter(GL_Framebuffer_SRGB)`  Blending is a linear operation, so we need to convert to linear, then back to sRGB. If `GFS` is enabled, if a destination image is sRGB, the color will be converted to linear, blended w/ linear source and convert back. If `GFS` is disabled, \"we assume the user knows what they are doing\"; thus, \"blending against an sRGB image will not perform any correction. This is usually not a good idea even if you are writing sRGB color values from the Fragment Shader.\"  [source](https://www.khronos.org/opengl/wiki/Framebuffer)  This entire section is summarized by this graphic:   @@im-full ![](/assets/gpu-colorspace.jpg) @@  # `clear_color()` vs `clear_color_srgb()`  If you call `clear_color_srgb()`, we enable `GFS` and record that in the global context if it wasn't already enabled. If you call `clear_color()`, we disable `GFS`  (and record that in the global context) if it wasn't already.  source: (`commit:filename:line-number)` `5d50e70:src/ops/clear.rs:38`  # Github Issues  - `#2059`: my issue, which this post hopefully addresses - `#1466`: requests a feature for `SrgbTexture2d` that `Texture2d` has - `#1414`: a misunderstanding - `#1185`: glutin issue - `#987`: created the `clear_color`–`clear_color_srgb` distinction  ## Issue `#1793`  - Setting `srgb=true` in `ContextBuilder` fixes the problem - claim: textures have sRGB and non-sRGB variants, so global context is not important  ## Issue `#1615`  - fix the problem with `program!()` macro in `src/macros.rs` using `let ___outputs_srgb: bool=false` - both variants of the `ProgramCreationInput` enum have `outputs_srgb` flags, and you can pass one of them to `Program::new()` to have direct control if you're not using `program!()` *(this is what I ended up fixing `#2059`, my issue, but I didn't understand it at the time)*  ## Issue `#805`  - some systems don't support `GFS` well, so it needs to be able to be disabled - rendering your image to an `SrgbTexture2d` instead of `Texture2d` fixes the problem - fix: a flag is available to `Program::new()` to turn it on and off *the same solution as `#1615` and `#2059` (mine)*  # What should we do?  We can either hide the fact that OpenGL uses `GFS`, or we should be very transparent about what `GFS` means and how to interact with it.  ## Transparent Approach  Here, `GFS` should be enabled or disabled when the context is initialized and never changed (there is already support for this in `ContextBuilder`). Using `clear_color()` to change the `GFS` value seems like the wrong way to go since it introduces unnecessary complexity. I can't think of a use case where being able to change `GFS` at runtime is useful if we're trying to be transparent (feel free to point one out in the comments).  In this approach with `GFS` enabled, we still require users to use `SrgbTexture`s and `Texture`s, and to differentiate what type of values are stored there. But, there is no need to specify whether a program outputs sRGB, since we are going to use OpenGL's underlying implicit conversions.  In this approach, if you choose to disable `GFS`:  - you should do everything using linear RGB and no gamma correction will be applied (except from `SrgbTexture`, which will of course be converted to linear when sampled from) - we assume the user understands what they are doing  ### How to implement this approach  First, remove `clear_color_srgb()` and don't fiddle with `GFS` in `clear_color()`. Second, remove the code which enables `GFS` in `use_program()` at `5d50e70:src/program/program.rs:449`.  ### Advantages of this approach  - familiarity: it respects the OpenGL model (using `GFS` as a global parameter) and obeys the [principle of least astonishment](https://en.wikipedia.org/wiki/Principle_of_least_astonishment)  - easy-of-use for beginners assuming they already understand OpenGL  ### Disadvantages  - blending with an sRGB image won't work nicely with `GFS` disabled, so all data must be in linear color space, which is mildly inconvenient  ## Hidden approach  The user should never have to worry about `GFS`. This is most similar to how things are done now. They do still need to worry about whether their textures and programs contain/output sRGB values.  ### How to implement this approach  We shouldn't let users choose the `GFS` setting when initializing a context since they shouldn't worry about it. And `GFS` should always be enabled. We will still need users to tell us whether they're texture has linear or sRGB values, and whether their program outputs linear or sRGB. But, that is the only time they'll need to worry about colorspace, we'll handle the enabling/disabling of `GFS` behind the scenes (similar to how we do it in `use_program()` at `5d50e70:src/program/program.rs:449`).  We also should remove `clear_color_srgb()` since users shouldn't worry about `GFS`.  ### Advantage of this approach  - simplicity: there's only 1 mode to worry about - backward compatibility with old `glium` programs since this is doesn't change the fundamental model  {{ addcomments }} "},
{title: "Voronoi Diagrams", content: "on the nearest post office. That map would partition the city into colored regions, which approximate a Voronoi diagram, a concept that often appears in science and engineering. These diagrams appear in the network of crystals in heated metal; the bubbles in soap foams; and disease outbreaks originating from different sources.  @@im-full ![](/assets/Euclidean_Voronoi_diagram.svg.png) @@  Motivated by Voronoi diagrams in the study of soap foams, I (along with my mentors at NIST) investigated the evolution of these diagrams over time. The potential energy of substances in a Voronoi format is often the surface area (or in the 2D case, perimeter). Thus, these diagrams often evolve by minimizing the total length of the boundaries of the regions (referred to as the perimeter). However, the problem becomes further complicated by requiring that none of the regions shrink and disappear, which often happens when this type of evolution is unconstrained. We can generalize the post office locations (the metaphor introduced in the first paragraph) as a collection of coordinates (referred to as sites). We are interested in an objective function whose input is the sites and whose value is the length of the perimeter of the diagram summed with an optional repulsion term, which keeps the sites from getting too close. This function is differentiable, so we can use many gradient descent methods to carry out the evolution. Interestingly, these different methods have different behavior. For example, using constant step-size gradient descent gives different behavior than using the Barzilai-Borwein method.  The objective function is highly nonlinear and nonconvex, so different methods and initial conditions yield different solutions. For each of these methods, we collect geometrical and topological statistics about the boundary network over time. Our goal is to understand how these diagrams change over time. Overall, it is important to understand the evolution of Voronoi diagrams because of the implications in metallurgy and the study of soap foams.  The source for this project can be found on [github](https://github.com/jasoneveleth/voronoi2).  {{ addcomments }} "},
{title: "Confetti", content: "ed to refresh my knowledge of camera matrices and rendering. So I created [this website](https://www.jasoneveleth.com/confetti) that displays an infinite stream of confetti. It calculates the movement of confetti in real-time. The code can be found [here](https://github.com/jasoneveleth/confetti).  # Design  I wanted physics and camera matrices to be the focus of the project, so the UI is spartan.  To describe what is happening, we will assume that in the world, $x,y,z$ are arranged as if we're looking at an $x$-$y$ graph, and $z$ is coming toward us:  @@im-full ![](/assets/axes.png) @@  We need to convert a point in that, to a point on the screen, which has coordinates $(0,0)$ in the top left, and increasing $x$ and $y$ going down and right. We can do that using `Rt`, which is the extrinsic matrix: $$[R\\mid t]=\\begin{bmatrix}1&0&0&t_x\\\\ 0&1&0&t_y\\\\ 0&0&1&t_z\\end{bmatrix}.$$ This matrix describes how the camera is positioned in relation to the origin. Our rotation is the identity, and our translation is $10$ in the $z$ direction (since our camera is oriented normally at $(0,0,-10)$). Thus, $t_x=t_y=0,t_z=10$. Left multiplying this matrix with a vector will move it 10 units in the $z$ direction (bringing the camera to the origin).  The second matrix we need is the intrinsic matrix: $$K=\\begin{bmatrix}f_x&s&u\\\\ 0&f_x&v\\\\ 0&0&s\\end{bmatrix}.$$ $f_x, f_y$ are the focal length in pixels of the camera. $u,v$ are the principle point offset (shift in the sensor inside the camera). $s$ is skew. For us, $f_x=f_y=\\max(w, h)/2$, which is the maximum of the width and height of the screen. Our skew is 1, and our principle point offsets are 0.  The `index.js` file holds most of the code, and `math.js` has some utility functions (like matrix multiplication). There is one function I want to highlight in `math.js`, and that is `world2image`. It converts world coordinates to image coordinates. It multiplies $K \\cdot [R\\mid t] \\cdot c$, where $c$ is the world coordinates and rescales the $x,y$ values by $z$. The `KRt` matrix is a precomputed multiplication of the intrinsic and extrinsic matrices and will take a point in 3D world coordinates and convert it to an $x,y$ coordinate system which is what the camera sees. Finally, we convert the $x,y$ coordinates to screen coordinates.  With the camera matrix calculation out of the way, let's dig into the implementation. I keep track of confetti using its center (in world coordinates), velocity, acceleration, and its two rotation parameters $\\theta$ and $\\phi$. I used a constant acceleration down at $-9.81$ (for obvious reasons). I initialize the position to be the center of the screen (the origin). I initialize the velocity randomly and uniformly from the top quadrant of a circle. I initialize the rotation parameters, and their deltas randomly.  The main approach is to keep a big array of confetti, and on each rendering step: - replace confetti that falls off-screen - update the world coordinates of each confetti (using acceleration, velocity, deltas for rotations, etc.) - extract the corners of each confetti given their orientation - use `world2image` to get the image coordinates of those corners - draw the quadrilaterals from those image coordinates.  Since we are calculating how each confetti would fall in real-time, large amounts of confetti (or badly optimized browsers), can really slow down a machine. Something to keep in mind if you use the website. Overall, I really enjoyed this project.  {{ addcomments }} "},
{title: "Why a Blog?", content: " to have a blog - I think some of the things I've learned could be useful to other people - Trying to explain something clarifies it in a way that keeping it to yourself doesn't - Other cool people have blogs and I want to be just like them  # What can you find on this blog?  - Computer science, math, and whatever I'm currently interested in  {{ addcomments }} "},
{title: "Hilarious Random Number Generator", content: "across a Facebook marketplace ad:  @@im-most ![](/assets/1story.png) @@  The website it linked to had a live updating \"people viewing this item\" count:  @@im-most ![](/assets/2story.png) @@  That seemed suspicious, so I looked into it.  @@im-most ![](/assets/3story.png) @@  @@im-most ![](/assets/4story.png) @@  They were just setting a random integer with jquery on a `setTimeout()`.   {{ addcomments }} "},
{title: "Bayes' Rule (in real life)", content: " no symptoms comes into your office after she's tested positive for breast cancer. Let's say the prevalence of breast cancer at her age is $1$ in $100$. Imagine we have a test that is $90\\%$ accurate when the person has cancer and $91\\%$ accurate when they don't have cancer.  We often refer to $90\\%$ as the \"sensitivity\" (since it's how *sensitive* the test is to the disease). We call the $91\\%$, \"specificity\", since it's how specific the test is for the disease.  The woman is stressed out that she tested positive. She asks you, \"How likely is it that I have breast cancer?\" **Is the answer 10% or 90%**? Many people make the mistake of saying $90\\%$. The actual probability is $10\\%$. This article will explain why, and how to avoid this mistake.  # A perspective on why this is wrong  My dad came up with a great way to think about it. Imagine a population of 1000 people and do the calculations on that. You know the prior is $1\\%$, so $10$ people have cancer, and $990$ people don't. Our test is $90\\%$ accurate when they have cancer; so, $9$ people who have cancer test positive, and $1$ person who has cancer tests negative. Of the $990$, our test is $91\\%$ accurate, so about $901$ people test negative, and about $89$ people test positive.  We know the woman tested positive, so she is either one of $9$ who has it, or $89$ who doesn't have it. $\\frac{9}{9+89}\\approx \\frac{1}{11}$ so she is about $10\\%$ likely to have cancer. That was a lot of calculation, and directly applying Bayes' rule isn't much simpler. We will create a framework for making these kinds of decisions that is mathematically sound, and easy to use in real life, even if you don't like [math](/404).  # An aside about odds  You may have heard your friends bet on something with $1:1$ odds (or even odds). This means the event has a $50\\%$ chance of happening. It is similar to the idea of \"parts\" in a cooking recipe, \"3 parts water\" and \"2 parts wine\" means the mixture will be $2:3=\\frac{2}{5}=40\\%$ wine. It's exactly the same happening here. $1:1$ is $50\\%$, $1:2$ is about $33.3\\%$, $1:3$ odds are $25\\%$. The idea is to go from the odds of: \"it happens $n$ times\": \"it happens $m$ times\", we go $$n:m = \\frac{\\text{times it happens}}{\\text{all the times}}=\\frac{n}{n+m}.$$ This is how you can convert odds to probability.  You can convert probability to odds using the following formula. Let's say the probability of an event $A$ happening is $P(A)$. The odds of it happening are $P(A):P(\\text{not } A).$  One nice thing about odds is you can cancel out common factors, so $2:4=1:2.$  # How do we fix our intuition?  You should think about the test---not as *determining* if she has breast cancer---but as *updating* your guess that she has breast cancer. Since the prevalence of breast cancer at her age (with no symptoms) is $1\\%,$ she has the odds of breast cancer of $1:99$.  Now, apply this formula (with $O$ representing odds): $$O_{new} = O_{old}O_{evidence}.$$ So, $$O_{new} = O_{old}O_{pos}= (1:99)(10:1) = 10:99 \\approx \\frac{1}{11}=0.09090909.$$ Thus, the chance that she has the disease is $10:99,$ about $\\textbf{9\\%}$. This is the key formula that you should use in your everyday life. It allows to you quickly incorporate more evidence into your calculation, or change your prior odds.  Where the hell does this come from? Don't worry, I will explain.  # You ask: Why is the formula you presented remotely true???  Intuitively, think about the [waterfall analogy](https://arbital.com/p/bayes_rule/?l=693)  @@im-full ![wide vs narrow waterfall](https://i.imgur.com/6FOndjc.png?0) @@  If the only thing we care about is the odds at the end (i.e. the ratio of red to blue at the bottom), then we only need to care about the odds that it goes in, $90:30$ or $45:15.$ Since they are the same odds, we end up with $3:4$ red to blue in both cases.  # The proof  We start with our prior odds, cancer:not cancer, which is written $O_{old}=P(C):P(\\lnot C).$   We want to know the probability of **cancer** *given* she **is positive**. In probability form, we write that $P(C\\mid +).$ This kind of conditional probability is really important to understand, and I can't explain it too thoroughly here, but the idea is when you say \"A given B\", you are restricting your universe of possible events to only ones where B is true, and you're asking, out of those times, how often is A true. That is, $P(A\\mid B) = \\frac{P(A\\cap B)}{P(B)}.$ We can rewrite this as $P(A\\mid B)P(B) = P(A\\cap B)$. This formula will be very important. You should also know that we can rename the A's to B's, so $P(B\\mid A)P(A) = P(B\\cap A)=P(A \\cap B)$. We will use this later.  The odds we want to know about can be written $O_{new} = P(C\\mid +):P(\\lnot C\\mid +)$. This will tell us our new estimate of the likelihood that the woman has cancer, given that she tested positive.  I claim that the question mark is true.   $$ \\begin{align*} &O_{old}O_{pos}\\\\&=(P(C):P(\\lnot C))\\cdot \\left(P(+\\mid C):P(+\\mid \\lnot C)\\right)\\\\ &\\stackrel{?}{=} P(C\\mid+):P(\\lnot C\\mid+)\\\\&=O_{new} \\end{align*} $$  Here is the proof: $$ \\begin{align*} (&P(C):P(\\lnot C))\\cdot \\left(P(+\\mid C):P(+\\mid \\lnot C)\\right)\\\\&=P(C)P(+\\mid C):P(\\lnot C)P(+\\mid \\lnot C)\\\\ &= P(+\\cap C):P(+\\cap \\lnot C) \\\\ &= P(+)P(C\\mid +):P(+)P(\\lnot C|+)\\\\ &= P(C\\mid +):P(\\lnot C|+)\\\\ \\end{align*} $$ ---  The last wrinkle to that calculation I showed earlier is how to calculate $O_{pos}=P(+\\mid C):P(+\\mid \\lnot C).$ The trick is that usually this is given to you or easy to calculate. $P(+\\mid \\lnot C)$ is the false positive rate, which if the test is $91\\%$ accurate when they don't have cancer is $9\\%$. And $P(+|C)$, which is the true positive rate, or sensitivity of the test, $90\\%$. Thus, the $O_{pos}=90:9=10:1.$  It should be clear now, that $$O_{new} = O_{old}O_{pos}= (1:99)(10:1) = 10:99 \\approx 9.2\\%.$$ This formula is amazing because it is very easy to add more evidence to update your understanding.  Let's say you reassure the woman. \"It was a routine checkup, you have no symptoms, we just need to run another test, it was probably a mistake.\" She does it again and comes back positive again. Now, she should start to get worried: $$O_{newer}=O_{new}O_{pos}=(10:99)(10:1)=(100:99)\\approx 50\\%.$$ Almost even odds. Okay, sure, but let's imagine that the second test came back negative. To understand the odds, we need to calculate the odds for a negative test. **Try this on your own and I'll see you in the next paragraph.**  Now, let's calculate this, $$O_{neg}=P(-\\mid C):P(-\\mid \\lnot C)=10:91.$$ This isn't as nice and round a  number, but it allows us to calculate: $$O = O_{old}O_{pos}O_{neg}=(1:99)(10:1)(10:91)=100:9009 \\approx 1\\%$$ Another great thing about this, is let's say we get more information that the prevalence in her age group is actually $1:10$. How do we incorporate that? Well, it replaces our prior estimate. So, $O_{old} = 1:10$ rather than $1:99$. Now, $$ \\begin{align*} O_{old}O_{pos} &=(1:10)(10:1)=10:10=50\\%\\\\ O_{old}O_{pos}O_{pos} &=(1:10)(10:1)(10:1)=100:1=99\\%\\\\ O_{old}O_{pos}O_{neg} &=(1:10)(10:1)(10:91)=100:910\\approx10\\% .\\end{align*} $$   # Unrelated: What does multiplying odds usually do?  The rule $(n:m) \\cdot (a:b) = an:mb,$ can be justified as follows:  $$ \\begin{align*} &(P(A):P(\\lnot A))(P(B):P(\\lnot B))\\\\ &= P(A)P(B):P(\\lnot A)P(\\lnot B)\\\\ &= P(A\\cap B):P(\\lnot A \\,\\cap \\,\\lnot B)\\\\ &= P(A\\cap B):P(\\lnot (A\\cup B)) \\end{align*} $$ This might be better understood by a very poorly drawn PowerPoint slide:   @@im-most ![](/assets/union-and-intersection.png) @@  {{ addcomments }} "},
{title: "Pick's Theorem", content: " is determined by this formula $$\\frac{B}{2} + I - 1.$$ Where $B$ is the number of vertices on the boundary (blue in the figure) and $I$ is the number of vertices in the interior of the shape (red in the figure).  @@im-full ![](/assets/picks_theorem1.jpeg) @@  # Proof  We will use induction on the area of the polygons.  ## Base Case  The base case is simply triangles that don't have any vertices on the interior.  It is easy to see that a isoceles right triangle with side length $1$, has area $\\frac{1}{2}$. This is clear from the formula for the area of the triangle ($\\frac{1}{2}bh$).  @@im-half ![|300](/assets/picks_theorem2.jpeg) @@  A shear takes the form:   $$\\begin{bmatrix}1 & 0\\\\ a & 1\\end{bmatrix}.$$ $$\\begin{vmatrix}1 & 0\\\\ a & 1\\end{vmatrix} = 1$$  So, shears preserve area, and thus, when a triangle is scaled by a shear, it's the area remains the same, so all triangles that are just sheared versions of each other have area $\\frac{1}{2}$.   ## Inductive Step    {{ addcomments }} "},
{title: "Five Color Theorem", content: " its vertices using at most 5 colors such that no two adjacent vertices have the same color.  ## Lemma: degree 5 vertex  @@im-half ![](/assets/five-color-theorem1.svg) @@  To show that every planar graph has at least 1 vertex of degree $\\leq 5$, we assume euler's formula, that given $F$ faces, $V$ vertices, and $E$ edges of a planar graph, that $$F + V = E + 2.$$  We assume for contradiction, there exists a planar graph $G = (F, V, E)$ with all vertices of degree $\\geq 6$. Then let us consider the size of this set:  $$VE = \\{(v,e) : v \\in V, e \\in E, v \\text{ is adjacent to } e\\}$$  Every edge will be in there twice, so $$|VE| = 2|E|.$$ Also, we know that all vertices have at least 6 edges, so $$|VE| \\geq 6|V|.$$ Thus, $$2|E| \\geq 6|V| \\implies \\frac{1}{3}|E| \\geq |V|.$$  Next, consider the set:  $$FE = \\{(f,e) : e \\in E, f \\in F, f \\text{ is adjacent to } e\\}$$  Every edge can see 2 faces, so $$|FE| = 2|E|,$$ and every face has at least 3 edges, so $$|FE| \\geq 3|F|.$$ Putting this together, $$2|E| \\geq 3|F| \\implies \\frac{2}{3}|E| \\geq |F|.$$  Now, we apply Euler's formula to get:  $$2 = F + V - E$$ $$\\leq \\frac{2}{3}|E| + \\frac{1}{3}|E| - |E|$$ $$= 0.$$  Thus, $2 \\leq 0$, a contradiction. This concludes the proof.  ---  ## Proof  Now for the real proof. This proof is by induction on the number of vertices in a planar graph:  Base case: $|V|=1$, it is clear that this is 5-colorable with just 1 color.  @@im-most  @@  Inductive step: We assume that all planar graphs with $|V|-1$ vertices are 5 colorable. We know from the lemma that there exists at least one vertex $w$ of degree 5 or less. We know that if it has less degree less than 5 that we instantly win because if we remove that vertex, it is 5 colorable (from our inductive hypothesis). And because $w$ has degree less than 5 it can be colored by a color that isn't in its neighbors. Thus, we can assume $w$ has 5 neighbors.  Now, because $w$ has 5 neighbors, if any two of them have the same color, then we can color $w$ the color that is missing, so we can assume that $w$ has degree at least 5.  To recap what we know so far: all planar graphs have at least one vertex with less than degree 5. The worst-case scenario for our proof is when it is exactly 5, and all of the neighbors have different colors. In all other cases, we win.  Now, we label the neighbors $v_1$ through $v_5$ based on their colors. We look at the largest subgraph that connects to vertex $1$ which only contains $1$ and $3$ colored vertices (it can have other colors hanging off of it). Now, if it doesn't contain $v_3$ then we can flip the color of each of the vertices of the subgraph (i.e. every $1$ colored vertex becomes a $3$ colored vertex). And then we can just color $w$ $1$ because it will have 2 vertices of color $3$ adjacent to it and no vertices of color $1$ now.  Thus, we can assume that there is an unbroken path from $v_1$ to $v_3$ which contains only $1$ and $3$ colored vertices. Now, we can apply the same exact logic to the colors $2$ and $4$. Thus, we only care about the case when there is an unbroken path from $v_2$ to $v_4$, as otherwise we can just flip the colors fo the subgraph coming from $v_2$ to recolor $w$ with color $2$. However, there can't be both an unbroken path from $v_1$ to $v_3$ using only $1$ and $3$ colored vertices, *and* an unbroken path from $v_2$ to $v_4$ using only $2$ and $4$ colored vertices. The paths have to cross because they are paths on a plane. This that case leads to a contradiction, so any planar graph must've fallen into one of the cases that was 5 colorable. Thus, every planar graph is 5 colorable. This concludes the proof.  ---  {{ addcomments }} "},
{title: "Linear Regression", content: "collection of coordinates.  For example, $\\{(x_1, y_1), (x_2,y_2), (x_3, y_3), \\dots\\}$. We would like to find $m,b$ such that $y = mx + b$ minimizes the sum of the residuals squared. By that I mean we want to minimize a function $L(\\alpha) = \\sum_i (y_i - (mx_i + b))^2$, by finding the best $m$ and $b$.  We will use the notation from [here](/404). Here are some fun facts we'll need to know: 1. $\\sum_i x_i^2 =\\lVert x \\rVert^2$ 2. $\\lVert x \\rVert^2 = x^Tx$ 3.  $(A + B)^T = A^T + B^T$ and $(AB)=B^TA^T$. 4. $D ( u^Tx) = u^T$, $D (x^Tu) = u^T$, and $D(x^Tx) = D(\\bar{x}^Tx) + D(x^T\\bar{x})$.[^1]  We need to set up some definitions. Let $$y = \\begin{bmatrix}y_1\\\\ y_2\\\\ y_3\\\\ y_4\\\\ \\vdots\\end{bmatrix}, X = \\begin{bmatrix}x_1 & 1\\\\ x_2 & 1\\\\ x_3 & 1\\\\ x_4 & 1\\\\ \\vdots & \\vdots\\end{bmatrix}, \\alpha = \\begin{bmatrix}m\\\\b\\end{bmatrix}.$$  So our goal is:  $$\\underset{\\alpha}{\\text{argmin}}\\, L(\\alpha)$$ $$=\\underset{\\alpha}{\\text{argmin}}\\, \\sum ((y_i) - (m x_i + b))^2$$ Apply fun fact number #1: $$=\\underset{\\alpha}{\\text{argmin}}\\, \\left\\lVert \\begin{bmatrix}(y_1 - (m x_1 + b))\\\\ (y_2 - (m x_2 + b))\\\\ (y_3 - (m x_3 + b))\\\\ (y_4 - (m x_4 + b))\\\\ \\vdots\\end{bmatrix}\\right\\rVert^2$$ $$=\\underset{\\alpha}{\\text{argmin}}\\, \\lVert y - X\\alpha\\rVert^2$$ Apply fun fact #2: $$=\\underset{\\alpha}{\\text{argmin}}\\, (y-X\\alpha)^T(y - X\\alpha)$$ Apply fun fact #3: $$=\\underset{\\alpha}{\\text{argmin}}\\, (y^T - \\alpha^TX^T)(y - X\\alpha)$$ $$=\\underset{\\alpha}{\\text{argmin}}\\, y^Ty - \\alpha^TX^Ty - y^TX\\alpha + \\alpha^TX^TX\\alpha$$  To minimize this, we find the critical points of the function. We do this by finding where the derivative of $L$ with respect to $\\alpha$ is $0$.  $$ \\begin{align*} 0 &= L'(\\alpha)\\\\ &= D (y^Ty - \\alpha^TX^Ty - y^TX\\alpha + \\alpha^TX^TX\\alpha)\\\\ &=D (y^Ty) - D (\\alpha^TX^Ty) - D (y^TX\\alpha) + D (\\alpha^TX^TX\\alpha)\\\\ \\end{align*}$$ Apply fun fact #4: $$ \\begin{align*} &= 0 - (X^Ty)^T - y^TX + D (\\alpha^TX^TX\\bar{\\alpha}) + D (\\bar{\\alpha}^TX^TX\\alpha)\\\\ &= -y^TX - y^TX + (X^TX\\alpha)^T + \\alpha^TX^TX\\\\ &=-2y^TX + \\alpha^T(X^TX)^T + \\alpha^TX^TX\\\\ &=-2y^TX + \\alpha^TX^TX + \\alpha^TX^TX\\\\ &=-2y^TX + 2\\alpha^TX^TX\\\\ \\end{align*} $$  And now for the glorious part (who am I kidding, this whole thing has been glorious),  $$2y^TX=2\\alpha^TX^TX$$ $$X^Ty=(X^TX)\\alpha$$ $$(X^TX)^{-1}X^Ty = \\alpha = \\begin{bmatrix}m & b\\end{bmatrix}$$  # Examples  Say we are given this set of coordinates: $\\{(1.3, 0.8), (3.2, 3.5), (5.6, 6.4), (8.5, 7.7)\\}$. Then we can either do the arithmetic by hand (from the last equation) or open `julia` and give it the vectors (some output omitted): julia julia> y = [0.8; 3.5; 6.4; 7.7] julia> X = [1.3 1; 3.2 1; 5.6 1; 8.5 1] julia> inv(X' * X) * X' * y 2-element Vector{Float64}:  0.9628  0.1228   Which gives us our desired line of best fit: $y = 0.962823x + 0.122874$.    [^1]: The bar in $\\bar{\\alpha}$ or $\\bar{x}$ means treat it as a constant, in the same way, that you would if you were doing the product rule with single-valued functions: $D(f(x)g(x)) = D(f(x)\\bar{g}(x)) + D(\\bar{f}(x) g(x)) = (Df(x))g(x) + (Dg(x))f(x)$.    {{ addcomments }} "},
]
</script>
<input oninput="search(this.value, mystuff)"/>
<div id="results-div"></div>
~~~
